# Configuration LM Studio pour AuraFlow

Ce guide vous explique comment configurer LM Studio pour fonctionner avec AuraFlow Web.

## üìã Pr√©requis

1. **LM Studio** install√© et configur√©
2. Au moins un mod√®le charg√© dans LM Studio
3. Le serveur local LM Studio d√©marr√©

## üöÄ Configuration rapide

### 1. D√©marrer LM Studio

1. Lancez **LM Studio**
2. Allez dans l'onglet **"Models"** et chargez un mod√®le compatible avec le chat (ex: Llama, Mistral, etc.)
3. Une fois le mod√®le charg√©, allez dans l'onglet **"Local Server"**
4. Cliquez sur **"Start Server"** (le bouton doit devenir vert)

### 2. Configuration de l'environnement

Cr√©ez un fichier `.env.local` dans votre projet ou modifiez le fichier `.env` existant :

```bash
# LM Studio Configuration
# Remplacez par l'IP de votre machine LM Studio
NEXT_PUBLIC_API_URL=http://192.165.2.65:1234

# Pour localhost (si LM Studio est sur la m√™me machine)
# NEXT_PUBLIC_API_URL=http://localhost:1234
```

**Notes importantes :**
- Par d√©faut, LM Studio √©coute sur le port `1234`
- Si LM Studio est sur une autre machine, utilisez son adresse IP locale
- Assurez-vous que le pare-feu autorise les connexions sur ce port

### 3. V√©rifier la connexion

1. D√©marrez votre application AuraFlow Web avec `pnpm dev`
2. Ouvrez la console de votre navigateur (F12)
3. Essayez d'envoyer un message
4. Vous devriez voir des messages de debug commen√ßant par "üéØ Using LM Studio provider"

## üîß Configuration avanc√©e

### Personnaliser les param√®tres

Vous pouvez cr√©er une instance personnalis√©e de LMStudioProvider :

```typescript
import { createLMStudioProvider } from '~/core/api/lm-studio';

const myLMStudio = createLMStudioProvider({
  baseURL: 'http://localhost:1234',
  model: 'llama-3.2-3b-instruct', // Utilisez le nom exact de votre mod√®le
  temperature: 0.8,
  maxTokens: 2048,
  stream: true
});
```

### Changer de mod√®le dynamiquement

```typescript
import { lmStudio } from '~/core/api/lm-studio';

// Obtenir la liste des mod√®les charg√©s
const models = await lmStudio.getLoadedModels();
console.log('Mod√®les disponibles:', models);

// Changer de mod√®le
await lmStudio.switchModel('nouveau-modele-id');
```

## üîç R√©solution des probl√®mes

### Message "Unexpected endpoint or method. (OPTIONS...)"

**Probl√®me :** LM Studio affiche des erreurs OPTIONS dans ses logs

**Explication :** C'est **NORMAL** ! Le navigateur envoie des requ√™tes CORS pr√©alables (preflight) qui g√©n√®rent ces messages. LM Studio r√©pond quand m√™me correctement avec un code 200.

**Solutions :**
- ‚úÖ **Ignorez ces messages**, ils n'emp√™chent pas le fonctionnement
- ‚úÖ Le chat devrait fonctionner malgr√© ces logs
- ‚úÖ C'est un comportement standard des navigateurs web

### Erreur de connexion (`ECONNREFUSED`)

**Probl√®me :** Ne peut pas se connecter √† LM Studio

**Solutions :**
1. ‚úÖ V√©rifiez que LM Studio est lanc√©
2. ‚úÖ V√©rifiez que le serveur local est d√©marr√© (bouton vert "Start Server")
3. ‚úÖ V√©rifiez l'URL dans `NEXT_PUBLIC_API_URL` (incluant l'IP correcte)
4. ‚úÖ V√©rifiez que le port n'est pas bloqu√© par un firewall
5. ‚úÖ Si LM Studio est sur une autre machine, v√©rifiez la connectivit√© r√©seau

### Erreur "No models loaded"

**Probl√®me :** Aucun mod√®le n'est charg√© dans LM Studio

**Solutions :**
1. ‚úÖ Allez dans l'onglet "Models" de LM Studio
2. ‚úÖ Chargez un mod√®le compatible avec le chat
3. ‚úÖ Attendez que le chargement soit termin√© (100%)
4. ‚úÖ V√©rifiez que vous avez suffisamment de RAM libre

### Erreur 500 (Server Error)

**Probl√®me :** Erreur interne du serveur LM Studio

**Solutions :**
1. ‚úÖ Red√©marrez le serveur LM Studio
2. ‚úÖ Rechargez le mod√®le
3. ‚úÖ V√©rifiez les logs de LM Studio pour des erreurs
4. ‚úÖ V√©rifiez les ressources syst√®me (RAM, CPU)

### R√©ponse lente ou timeout

**Probl√®me :** Les r√©ponses sont tr√®s lentes

**Solutions :**
1. ‚úÖ Utilisez un mod√®le plus petit (ex: 7B au lieu de 70B)
2. ‚úÖ R√©duisez `maxTokens` dans la configuration
3. ‚úÖ Fermez d'autres applications consommatrices de RAM
4. ‚úÖ Utilisez le mode non-streaming si n√©cessaire : `stream: false`

## üéØ Mod√®les recommand√©s

### Pour un usage rapide (< 8GB RAM)
- **Llama 3.2 3B Instruct** - √âquilibr√©
- **Phi-3 Mini** - Tr√®s rapide
- **Qwen2.5 7B Instruct** - Bonne qualit√©

### Pour une meilleure qualit√© (8-16GB RAM)
- **Llama 3.1 8B Instruct** - Excellent √©quilibre
- **Mistral 7B Instruct** - Tr√®s polyvalent
- **CodeLlama 7B** - Optimis√© pour le code

### Pour la meilleure qualit√© (16GB+ RAM)
- **Llama 3.1 70B Instruct** (quantis√©)
- **Mixtral 8x7B** - Tr√®s performant
- **CodeLlama 34B** - Excellent pour le code

## üö® Notes importantes

1. **M√©moire :** Assurez-vous d'avoir suffisamment de RAM libre pour le mod√®le choisi
2. **Mod√®les :** Seuls les mod√®les de type "Instruct" ou "Chat" fonctionnent bien pour la conversation
3. **Quantification :** Les mod√®les quantifi√©s (Q4, Q8) utilisent moins de RAM mais peuvent √™tre l√©g√®rement moins pr√©cis
4. **Streaming :** Le streaming permet une r√©ponse progressive, mais peut √™tre d√©sactiv√© si probl√©matique

## üìû Support

Si vous rencontrez des probl√®mes :

1. V√©rifiez les logs dans la console du navigateur
2. V√©rifiez les logs de LM Studio
3. Testez avec l'interface web de LM Studio (http://localhost:1234)
4. Essayez avec un mod√®le diff√©rent
